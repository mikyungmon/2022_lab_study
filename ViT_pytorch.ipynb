{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ad9105",
   "metadata": {},
   "source": [
    "# ViT(Vision Transformer)\n",
    "\n",
    "https://github.com/lukemelas/PyTorch-Pretrained-ViT 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9826d",
   "metadata": {},
   "source": [
    "### utils - helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e68ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Optional   # 변수에 None이 들어올 수도 있으면 Optional을 선언하여 사용함\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# from torchsummary import summary\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummaryX import summary\n",
    "import numpy as np\n",
    "from torch.utils import model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78fd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, model_name=None, weights_path=None, load_first_conv=True, load_fc=True, load_repr_layer=False, \n",
    "                           resize_positional_embedding=False, verbose = True, strict=True,):\n",
    "    assert bool(model_name) ^ bool(weights_path), 'expected exactly one of model_name or weights_path'\n",
    "    \n",
    "    # load or download weights\n",
    "    if weights_path is None:\n",
    "        url = PRETRAINED_MODELS[model_name]['url']\n",
    "        if url:\n",
    "            state_dict = model_zoo.load_url(url)   # model_zoo.load_url - 주어진 url에서 torch 직렬화된 개체를 로드\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'pretrained model for {model_name} has not yet been released')   # raise - 예외 에러 발생 시키기, ValueError은 키워드\n",
    "            \n",
    "    else:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        \n",
    "    # modifications to load partial state dict(부분 state dict을 load하기 위한 수정)\n",
    "    expected_missing_keys = []\n",
    "    if not load_first_conv and 'patch_embedding.weight' in state_dict:\n",
    "        expected_missing_keys +=['patch_embedding.weight', 'patch_embedding.bias']\n",
    "    \n",
    "    if not load_fc and 'fc.weight' in state_dict:\n",
    "        expected_missing_keys +=['fc_weight', 'fc.bias']\n",
    "        \n",
    "    if not load_repr_layer and 'pre_logits.weight' in state_dict:\n",
    "        expected_missing_keys +=['pre_logits.weight', 'pre_logits.bias']\n",
    "        \n",
    "    for key in expected_missing_keys:\n",
    "        state_dict.pop(key)    # pop - 리스트의 마지막 요소 꺼내고 삭제\n",
    "        \n",
    "    # change size of positional embeddings\n",
    "    if resize_positional_embedding:\n",
    "        posemb = state_dict['positional_embeddings.pos_embedding']   # static_dict은 torch.save처럼 모델 저장, 각 layer마다 텐서로 매핑되는 매개변수(예를 들어 가중치, 편향 등)를 python dictionary 타입으로 저장한 객체(한마디로 모델 구조에 맞게 각 레이어마다 매개변수를 텐서형태로 매핑해서 dictionary 형태로 저장하는 것)\n",
    "        posemb_new = model.state_dict()['positional_embedding.pos_embedding']\n",
    "        state_dict['positional_embedding.pos_embedding'] = \\resize_positional_embedding(posemb=posemb, posemb_new=posemb_new,\n",
    "                                                                                        has_class_token=hasattr(model, 'class_token'))\n",
    "        maybe_print('Resized positional embeddings from {} to {}'. format(posemb.shape, posemb_new.shape), verbose)\n",
    "        \n",
    "    # load state dict (파라미터 값을 모델에게 입히는 작업,,(?))\n",
    "    ret = model.load_state_dict(state_dict, strict=False)\n",
    "    if strict:\n",
    "        assert set(ret.missing_keys) == set(expected_missing_keys), \\'missing keys when loading pretrained weights:{}'.format(ret.missing_keys)\n",
    "        assert not ret.unexpected_keys, \\'missing keys when loading pretrined weights: {}'.format(ret.unexpected_keys)\n",
    "        maybe_print('loaded pretrined weights.', verbose)\n",
    "    \n",
    "    else:\n",
    "        maybe_print('missing keys when loading pretrained weights.{}'.format(ret.missing_keys), verbose)\n",
    "        maybe_print('unexpected keys when loading pretrained weights:{}'.format(ret.unexpected_keys), verbose)\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def resize_positional_embedding:\n",
    "    from scipy.ndimage import zoom\n",
    "    \n",
    "    # deal with class token\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if has_class_token:\n",
    "        posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "        ntok_new -=1\n",
    "        \n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:,:0], posemb[0]\n",
    "        \n",
    "    # get old and new grid sizes\n",
    "    gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "    gs_new = int(np.sqrt(ntok_new))\n",
    "    posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)   # 불러온 그리드로 reshape해주고?\n",
    "    \n",
    "    # rescale grid\n",
    "    zoom_factor = (gs_new / gs_old, gs_new / gs_old,1)\n",
    "    posemb_grid = zoom(posemb_grid, zoom_factor, order =1)   # zoom은 보간해주는거. 줌 패턴으로 그리드를 늘려주고, \n",
    "    posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "    posemb_grid = torch.from_numpy(posemb_grid)\n",
    "    \n",
    "    # deal with class token and return \n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    \n",
    "    return posemb\n",
    "    \n",
    "\n",
    "def as_tuple(x):\n",
    "    return x if isinstance(x, tuple) else (x,x)    # isinstance(x,tuple) - x가 tuple인지 확인\n",
    "\n",
    "\n",
    "def maybe_print(s: str, flag: bool):\n",
    "    if flag:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9139c4a0",
   "metadata": {},
   "source": [
    "### transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_last(x,shape):  # split the last dimension to given shape\n",
    "    shape = list(shape)\n",
    "    assert shape.count(-1) <=1   # shape.count(-1) : shape list에 -1 개수 count하는거, -1이 1개 이상있으면 오류\n",
    "    if -1 in shape:\n",
    "        shape[shape.index(-1)] = int(x.size(-1)/ -np.prod(shape))\n",
    "        \n",
    "    return x.view(*x.size()[:-1], *shape)\n",
    "\n",
    "def merge_last(x, n_dims):   # merge the last n_dims to a dimension\n",
    "    s = x.size()\n",
    "    assert n_dims > 1 and n_dims < len(s)\n",
    "    \n",
    "    return x.view(*s[:-n_dims],-1)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.proj_q = nn.Linear(dim, dim)\n",
    "        self.proj_k = nn.Linear(dim, dim)\n",
    "        self.proj_v = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.n_heads = num_heads\n",
    "        self.scores = None   # for visualization\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        q,k,v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
    "        q,k,v = (split_last(x, (self.n_heads, -1)).transpose(1,2) for x in [q,k,v])\n",
    "        scores = q @ k.transpose(-2,-1) / np.sqrt(k.size(-1))\n",
    "        if mask in not None:\n",
    "            mask = mask[:, None,None, :].float()\n",
    "            scores -=1000.0 * (1.0 - mask)\n",
    "        scores = self.drop(F.softmax(scores, dim=-1))\n",
    "        h =  (scores @ v).transpose(1,2).contiguous()\n",
    "        h = merge_last(h,2)\n",
    "        self.scores = scores\n",
    "        \n",
    "        return h\n",
    "    \n",
    "class PositionalWiseFeedForward(nn.Module):\n",
    "    def __init__(self, dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedSelfAttention(dim, num_heads, dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps = 1e-6)\n",
    "        self.pwff = PositionWIiseFeedForward(dim,ff_dim)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        h = self.drop(self.proj(self.attn(self.norm1(x), mask)))\n",
    "        x = x + h\n",
    "        h = self.drop(self.pwff(self.norm2(x)))\n",
    "        x = x + h\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(dim,num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414371d",
   "metadata": {},
   "source": [
    "### configs - ViT model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"configs.py - ViT model configurations, based on:\n",
    "https://github.com/google-research/vision_transformer/blob/master/vit_jax/configs.py\n",
    "\"\"\"\n",
    "\n",
    "def get_base_config():\n",
    "    \"\"\"Base ViT config ViT\"\"\"\n",
    "    return dict(\n",
    "      dim=768,\n",
    "      ff_dim=3072,\n",
    "      num_heads=12,\n",
    "      num_layers=12,\n",
    "      attention_dropout_rate=0.0,\n",
    "      dropout_rate=0.1,\n",
    "      representation_size=768,\n",
    "      classifier='token'\n",
    "    )\n",
    "\n",
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = get_base_config()\n",
    "    config.update(dict(patches=(16, 16)))\n",
    "    return config\n",
    "\n",
    "def get_b32_config():\n",
    "    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n",
    "    config = get_b16_config()\n",
    "    config.update(dict(patches=(32, 32)))\n",
    "    return config\n",
    "\n",
    "def get_l16_config():\n",
    "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
    "    config = get_base_config()\n",
    "    config.update(dict(\n",
    "        patches=(16, 16),\n",
    "        dim=1024,\n",
    "        ff_dim=4096,\n",
    "        num_heads=16,\n",
    "        num_layers=24,\n",
    "        attention_dropout_rate=0.0,\n",
    "        dropout_rate=0.1,\n",
    "        representation_size=1024\n",
    "    ))\n",
    "    return config\n",
    "\n",
    "def get_l32_config():\n",
    "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
    "    config = get_l16_config()\n",
    "    config.update(dict(patches=(32, 32)))\n",
    "    return config\n",
    "\n",
    "def drop_head_variant(config):\n",
    "    config.update(dict(representation_size=None))\n",
    "    return config\n",
    "\n",
    "\n",
    "PRETRAINED_MODELS = {\n",
    "    'B_16': {\n",
    "      'config': get_b16_config(),\n",
    "      'num_classes': 21843,\n",
    "      'image_size': (224, 224),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16.pth\"\n",
    "    },\n",
    "    'B_32': {\n",
    "      'config': get_b32_config(),\n",
    "      'num_classes': 21843,\n",
    "      'image_size': (224, 224),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_32.pth\"\n",
    "    },\n",
    "    'L_16': {\n",
    "      'config': get_l16_config(),\n",
    "      'num_classes': 21843,\n",
    "      'image_size': (224, 224),\n",
    "      'url': None\n",
    "    },\n",
    "    'L_32': {\n",
    "      'config': get_l32_config(),\n",
    "      'num_classes': 21843,\n",
    "      'image_size': (224, 224),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/L_32.pth\"\n",
    "    },\n",
    "    'B_16_imagenet1k': {\n",
    "      'config': drop_head_variant(get_b16_config()),\n",
    "      'num_classes': 1000,\n",
    "      'image_size': (384, 384),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16_imagenet1k.pth\"\n",
    "    },\n",
    "    'B_32_imagenet1k': {\n",
    "      'config': drop_head_variant(get_b32_config()),\n",
    "      'num_classes': 1000,\n",
    "      'image_size': (384, 384),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_32_imagenet1k.pth\"\n",
    "    },\n",
    "    'L_16_imagenet1k': {\n",
    "      'config': drop_head_variant(get_l16_config()),\n",
    "      'num_classes': 1000,\n",
    "      'image_size': (384, 384),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/L_16_imagenet1k.pth\"\n",
    "    },\n",
    "    'L_32_imagenet1k': {\n",
    "      'config': drop_head_variant(get_l32_config()),\n",
    "      'num_classes': 1000,\n",
    "      'image_size': (384, 384),\n",
    "      'url': \"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/L_32_imagenet1k.pth\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83033b",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e4b1a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeding1D(nn.Module):\n",
    "    def __init__(self,seq_len,dim):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1,seq_len,dim))    # 0으로 이루어진 1,seq_len 크기의 텐서\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x + self.pos_embedding  # 이 x는 뭔지..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, name:Optional[srt]=None, pretrained: bool=False, patches: int=16, dim: int=768, ff_dim: int=3072, num_heads: int=12, \n",
    "                 attention_dropout_rate: float=0.1, representation_size : Optional[int] = None, load_repr_layer: bool=False, classifier: str='token',\n",
    "                 positional_embedding: str='1d', in_channels: int=3, image_size: Optional[int] = None, num_classes: Optional[int]=None,):\n",
    "        super().__init__()\n",
    "        \n",
    "        if name is None:\n",
    "            check_msg = 'must specify name of pretrained model'\n",
    "            assert not pretrained, check_msg    # assert - 가정 설정문, 뒤의 조건이 true가 아니면 AssertError발생시킴\n",
    "            assert not resize_positional_embedding, check_msg   # assert 부분 의미를 모르겠음\n",
    "            if num_classes is None:\n",
    "                num_classes = 1000\n",
    "            if image_size is None:\n",
    "                image_size = 384\n",
    "        \n",
    "        else:  # load pretrained model\n",
    "            assert name in PRETRAINED_MODELS.keys(), \\'name should be in: ' + ', '.join(PRETRAINED_MODELS.keys())\n",
    "            config = PRETRAINED_MODELS[name]['config']\n",
    "            patches = config['patches']\n",
    "            dim = config['dim']\n",
    "            ff_dim = config['ff_dim']\n",
    "            num_heads = config['num_heads']\n",
    "            num_layers = config['num_layers']\n",
    "            attention_dropout_rate = config['attention_dropout_rate']\n",
    "            dropout_rate = config['dropout_rate']\n",
    "            representation_size = config['representation_size']\n",
    "            classifier = config['classifier']\n",
    "            \n",
    "            if image_size is None:\n",
    "                image_size = PRETRAINED_MODELS[name]['image_size']\n",
    "            \n",
    "            if num_classes is None:\n",
    "                num_classes = PRETRAINED_MODELS[name]['num_classes']\n",
    "                \n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # image & patch sizes\n",
    "        h,w = as_tuple(image_size)\n",
    "        fh,fw = as_tuple(patches)\n",
    "        gh,gw = h//fh, w//fw   # patch 갯수\n",
    "        seq_len = gh * gw\n",
    "        \n",
    "        # patch embedding\n",
    "        self.patch_embedding = nn.Conv2d(in_channels, dim, kernel_size=(fh,fw), stride = (fh,fw))\n",
    "        \n",
    "        # class token\n",
    "        if classifier == 'token':\n",
    "            self.class_token = nn.Parameter(torch.zeros(1,1,dim))\n",
    "            seq_len +=1\n",
    "            \n",
    "        # positional embedding\n",
    "        if positional_embedding.lower() == '1d':\n",
    "            self.positional_embedding = PositionalEmbedding1D(seq_len, dim)\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "        # transformer\n",
    "        self.transformer = Transformer(num_layers=num_layers, dim=dim, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout_rate)\n",
    "        \n",
    "        # representation layer\n",
    "        if representation_size and load_repr_layer:\n",
    "            self.pre_logits = nn.Linear(dim, representation_size)\n",
    "            pre_logits_size = representation_size\n",
    "        else:\n",
    "            pre_logits_size = dim\n",
    "            \n",
    "        # classifier head\n",
    "        self.norm = nn.LayerNorm(pre_logits_size, eps = 1e-6)\n",
    "        self.fc = nn.Linear(pre_logits_size, num_classes)\n",
    "        \n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "        # load pretrained model\n",
    "        if pretrained:\n",
    "            pretrained_num_channels = 3\n",
    "            pretrained_num_classes = PRETRAINED_MODELS[name]['num_classes']\n",
    "            pretrained_image_size = PRETRAINED_MODELS[name]['image_size']\n",
    "            load_pretrained_weights(self, name, load_first_conv=(in_channels==pretrained_num_channels), load_fc = (num_classes= pretrained_num_classes), \n",
    "                                   load_repr_layer = load_repr_payer, resize_positional_embedding=(image_size !=pretrained_image_size),)\n",
    "            \n",
    "        def init_weights(self):\n",
    "            def _init(m):\n",
    "                if isinstance(m, nn.Linear):   # m이 nn.linear인지 확인..?\n",
    "                    nn.init.xavier_uniform_(m.weight)  # _trunc_normal(m.weight, std=0.02)  # from .initialization import _trunc_normal\n",
    "                    if hasattr(m, 'bias') and m.bias is not None:   # hasattr- 변수가 있는지 확인함. ex) hasattr(cls,'b') -> cls에 b라는 멤버가 있는지 확인\n",
    "                        nn.init.normal_(m.bias, std=1e-6)  # nn.init.constant(m.bias, 0)\n",
    "            self.apply(_init)\n",
    "            nn.init.constant_(self.fc.weight, 0)\n",
    "            nn.init.constant_(self.fc.bias, 0)\n",
    "            nn.init.normal_(self.positional_embedding.pos_embedding, std=0.02)  # _trunc_normal(self.positional_embedding.pos_embedding, std=0.02)\n",
    "            nn.init.constant_(self.class_token, 0)\n",
    "            \n",
    "        def forward(self,x):\n",
    "            batch, channel, fh,fw = x.shape\n",
    "            x = self.patch_embdding(x)    # b,d,gh,gw\n",
    "            x = x.flatten(2).transpose(1,2)    # b,gh*gw,d\n",
    "            if hasattr(self, 'class_token'):\n",
    "                x = torch.cat((self.class_token.expand(b,-1,-1),x), dim=1)\n",
    "            if hasattr(self,'positional_embedding'):\n",
    "                x = self.positional_embedding(x)\n",
    "            x = self.transformer(x)\n",
    "            if hasattr(self,'pre_logits'):\n",
    "                x = self.pre_logits(x)\n",
    "                x = torch.tanh(x)\n",
    "            if hasattr(self,'fc'):\n",
    "                x = self.norm(x)[:,0]\n",
    "                x = self.fc(x)\n",
    "            \n",
    "            return x\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ff3c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abb61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e21a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962f69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9739310a",
   "metadata": {},
   "source": [
    "=================================================================\n",
    "\n",
    "여기부터는 https://github.com/FrancescoSaverioZuppichini/ViT 해당 링크 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a53e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('E:\\\\Transformer\\\\Vision\\\\cat_img.jpg')\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.imshow(img_resize)\n",
    "transform = Compose([Resize((224, 224)), ToTensor()])\n",
    "x = transform(img)\n",
    "x = x.unsqueeze(0) # add batch dim\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa384d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16 # 16 pixels\n",
    "pathes = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f950b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size : int = 16, emb_size : int = 768, img_size : int = 224):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(nn.Conv2d(in_channels, emb_size, kernel_size = patch_size, stride = patch_size),Rearrange('b e (h) (w) -> b (h w) e'),)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))   # torch.randn : 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용해 생성\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size)**2 +1, emb_size))  # (img_size // patch_size)**2는 패치 갯수인데 +1은 왜하는지 모르겠음, 1은 cls추가 해준거 때문에 하는거?\n",
    "        \n",
    "    def forward(self, x:Tensor) -> Tensor :\n",
    "        print('x:',x)\n",
    "        b, _, _, _ = x.shape   # batch size\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)   # cls_token을 반복하여 batch size와 크기 맞춰줌\n",
    "        x = torch.cat([cls_tokens,x], dim=1)    # cls token을 input에 추가(concat)\n",
    "        x += self.positions    # position embedding 더해줌\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6466e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[[0.8471, 0.8588, 0.8745,  ..., 0.9098, 0.9098, 0.9098],\n",
      "          [0.8471, 0.8588, 0.8745,  ..., 0.9176, 0.9137, 0.9137],\n",
      "          [0.8471, 0.8588, 0.8745,  ..., 0.9176, 0.9137, 0.9137],\n",
      "          ...,\n",
      "          [0.7490, 0.7608, 0.7412,  ..., 0.8667, 0.8667, 0.8627],\n",
      "          [0.7490, 0.7529, 0.7412,  ..., 0.8745, 0.8784, 0.8824],\n",
      "          [0.7333, 0.7412, 0.7333,  ..., 0.8784, 0.8824, 0.8863]],\n",
      "\n",
      "         [[0.8235, 0.8353, 0.8471,  ..., 0.8706, 0.8706, 0.8706],\n",
      "          [0.8235, 0.8353, 0.8471,  ..., 0.8784, 0.8745, 0.8706],\n",
      "          [0.8235, 0.8353, 0.8471,  ..., 0.8784, 0.8706, 0.8588],\n",
      "          ...,\n",
      "          [0.6157, 0.6275, 0.6118,  ..., 0.8157, 0.8157, 0.8118],\n",
      "          [0.6157, 0.6235, 0.6118,  ..., 0.8235, 0.8275, 0.8314],\n",
      "          [0.6039, 0.6118, 0.6078,  ..., 0.8275, 0.8314, 0.8353]],\n",
      "\n",
      "         [[0.6902, 0.7098, 0.7373,  ..., 0.8353, 0.8353, 0.8353],\n",
      "          [0.6902, 0.7098, 0.7373,  ..., 0.8392, 0.8353, 0.8314],\n",
      "          [0.6902, 0.7059, 0.7294,  ..., 0.8314, 0.8275, 0.8157],\n",
      "          ...,\n",
      "          [0.4706, 0.4863, 0.4745,  ..., 0.7529, 0.7529, 0.7490],\n",
      "          [0.4784, 0.4902, 0.4824,  ..., 0.7608, 0.7647, 0.7686],\n",
      "          [0.4706, 0.4824, 0.4784,  ..., 0.7647, 0.7686, 0.7725]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PatchEmbedding()(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0f92b6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8471, 0.8588, 0.8745,  ..., 0.9098, 0.9098, 0.9098],\n",
      "          [0.8471, 0.8588, 0.8745,  ..., 0.9176, 0.9137, 0.9137],\n",
      "          [0.8471, 0.8588, 0.8745,  ..., 0.9176, 0.9137, 0.9137],\n",
      "          ...,\n",
      "          [0.7490, 0.7608, 0.7412,  ..., 0.8667, 0.8667, 0.8627],\n",
      "          [0.7490, 0.7529, 0.7412,  ..., 0.8745, 0.8784, 0.8824],\n",
      "          [0.7333, 0.7412, 0.7333,  ..., 0.8784, 0.8824, 0.8863]],\n",
      "\n",
      "         [[0.8235, 0.8353, 0.8471,  ..., 0.8706, 0.8706, 0.8706],\n",
      "          [0.8235, 0.8353, 0.8471,  ..., 0.8784, 0.8745, 0.8706],\n",
      "          [0.8235, 0.8353, 0.8471,  ..., 0.8784, 0.8706, 0.8588],\n",
      "          ...,\n",
      "          [0.6157, 0.6275, 0.6118,  ..., 0.8157, 0.8157, 0.8118],\n",
      "          [0.6157, 0.6235, 0.6118,  ..., 0.8235, 0.8275, 0.8314],\n",
      "          [0.6039, 0.6118, 0.6078,  ..., 0.8275, 0.8314, 0.8353]],\n",
      "\n",
      "         [[0.6902, 0.7098, 0.7373,  ..., 0.8353, 0.8353, 0.8353],\n",
      "          [0.6902, 0.7098, 0.7373,  ..., 0.8392, 0.8353, 0.8314],\n",
      "          [0.6902, 0.7059, 0.7294,  ..., 0.8314, 0.8275, 0.8157],\n",
      "          ...,\n",
      "          [0.4706, 0.4863, 0.4745,  ..., 0.7529, 0.7529, 0.7490],\n",
      "          [0.4784, 0.4902, 0.4824,  ..., 0.7608, 0.7647, 0.7686],\n",
      "          [0.4706, 0.4824, 0.4784,  ..., 0.7647, 0.7686, 0.7725]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_embedded = PatchEmbedding()(x)\n",
    "TransformerEncoderBlock()(patches_embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "35427be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size : int = 768, num_heads: int = 8, dropout: float=0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self,x: Tensor,mask: Tensor=None)->Tensor: \n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)  # batch, heads, sequence_len, embedding_size 모양으로 변경\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values  = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd,bhkd -> bhqk', queries, keys)   # 쿼리와 키 곱함. einops 이용해 자동으로 transpose 후 내적이 진행됨, 결과 벡터 모양은 batch, heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill()\n",
    "            \n",
    "        scaling = self.emb_size**(1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling  \n",
    "        att = self.att_drop(att)\n",
    "        \n",
    "        # 세 번째 축에 대한 합산\n",
    "        out = torch.einsum('bhal, bhlv -> bhav', att, values)  # scaling해준 후 얻어진 attention score와 밸류를 내적\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')   # emb_size로 rearrange하면 MHA의 output나옴\n",
    "        out = self.projection(out)   # 최종 output은 linear layer거쳐서 나오게 됨\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7f58b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self,fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self,x, **kwargs):    # **kwargs는 keyword argument의 줄임말로 키워드를 제공.딕셔너리 형태로 {'키워드':'특정 값'} 이렇게 함수내부로 전달됨\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5b91e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA 이후 진행되는 MLP 부분\n",
    "class FeedForwardBlock(nn.Sequential):  \n",
    "    def __init__(self, emb_size: int, expansion: int=4, drop_p: float= 0.):\n",
    "        super().__init__(nn.Linear(emb_size, expansion * emb_size), nn.GELU(), nn.Dropout(drop_p), nn.Linear(expansion*emb_size, emb_size),)    # expansion * emb_size가 hidden_layer 수 말하는듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e2ca9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int=768, drop_p: float=0., forward_expansion: int=4, forward_drop_p: float=0., **kwargs):\n",
    "        super().__init__(ResidualAdd(nn.Sequential(nn.LayerNorm(emb_size), MultiHeadAttention(emb_size, **kwargs), nn.Dropout(drop_p))),\n",
    "                         ResidualAdd(nn.Sequential(nn.LayerNorm(emb_size), FeedForwardBlock(emb_size, expansion = forward_expansion, drop_p = forward_drop_p), nn.Dropout(drop_p))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "69114bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int=12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "abfff779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int=768, n_classes: int=1000):\n",
    "        super().__init__(Reduce('b n e -> b e', reduction = 'mean'), nn.LayerNorm(emb_size), nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ab9eec4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6508/2344826990.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                          ClassificationHead(emb_size, n_classes))\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mViT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: array() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self, in_channels: int=3, patch_size: int=16, emb_size: int=768, img_size: int=224, depth: int=12, n_classes: int=1000, **kwargs):\n",
    "        super().__init__(PatchEmbedding(in_channels, patch_size, emb_size, img_size), TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "                         ClassificationHead(emb_size, n_classes))\n",
    "        \n",
    "summary(ViT(), numpy.array(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263346cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71728f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2ede336",
   "metadata": {},
   "source": [
    "# 참고\n",
    "\n",
    "https://github.com/lukemelas/PyTorch-Pretrained-ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1fb32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch(practice)",
   "language": "python",
   "name": "pytorch_3min"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
                                                                                                                                                                                                                                                                                                                                                                                                                   img = Image.open(self.img_list[idx])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e252516",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((384,384)),transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.Resize((384,384)),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c1b2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "torch.Size([3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "train_covid_1 = CovidDataset(transform=train_transform, file_path= train_covid_path_1)\n",
    "train_covid_2 = CovidDataset(transform=train_transform, file_path= train_covid_path_2)\n",
    "train_covid_3 = CovidDataset(transform=train_transform, file_path= train_covid_path_3)\n",
    "test_covid_1 = CovidDataset(transform=test_transform, file_path= test_covid_path_1)\n",
    "test_covid_2 = CovidDataset(transform=test_transform, file_path= test_covid_path_2)\n",
    "test_covid_3 = CovidDataset(transform=test_transform, file_path= test_covid_path_3)\n",
    "train_normal =CovidDataset(transform=train_transform, file_path= train_normal_path)\n",
    "test_normal = CovidDataset(transform=test_transform, file_path= test_normal_path)\n",
    "print(train_covid_1[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c46cdba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "train = train_covid_1+ train_covid_2 + train_covid_3 + train_normal\n",
    "test = test_covid_1+ test_covid_2 + test_covid_3 + test_normal\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0bb417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train,batch_size = 30, shuffle=True, num_workers=0)\n",
    "test_data_loader = DataLoader(test,batch_size=30, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b549d8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_loader.dataset)) \n",
    "print(len(test_data_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dffa69",
   "metadata": {},
   "source": [
    "### train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33e98ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'PatchEmbedding' object has no attribute 'pos_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28824/3022784169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model_vit = ViT(name ='B_16_imagenet1k', pretrained=True, emb_size=768, ff_dim=3072, num_heads=12, \n\u001b[0m\u001b[0;32m      2\u001b[0m                  attention_dropout_rate=0.0, dropout_rate = 0.1, in_channels=3,image_size=384, depth=12, n_classes=2 )\n\u001b[0;32m      3\u001b[0m \u001b[0mUSE_CUDA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# cuda를 쓸 수 있는지 확인하는 코드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# use_cude의 결과에 따라 데이터를 cuda 혹은 cpu로 보내도록 가리키는 역할\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# batch_size = 64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28824/2270919242.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, pretrained, patch_size, emb_size, ff_dim, num_heads, attention_dropout_rate, dropout_rate, in_channels, image_size, depth, n_classes)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# initialize weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# load pretrained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28824/2270919242.py\u001b[0m in \u001b[0;36minit_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# _trunc_normal(self.positional_embedding.pos_embedding, std=0.02)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch_3min\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m    779\u001b[0m             type(self).__name__, name))\n\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'PatchEmbedding' object has no attribute 'pos_embedding'"
     ]
    }
   ],
   "source": [
    "model_vit = ViT(name ='B_16_imagenet1k', pretrained=True, emb_size=768, ff_dim=3072, num_heads=12, \n",
    "                 attention_dropout_rate=0.0, dropout_rate = 0.1, in_channels=3,image_size=384, depth=12, n_classes=2 )\n",
    "USE_CUDA = torch.cuda.is_available()   # cuda를 쓸 수 있는지 확인하는 코드\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")  # use_cude의 결과에 따라 데이터를 cuda 혹은 cpu로 보내도록 가리키는 역할\n",
    "# batch_size = 64   \n",
    "epochs = 40    \n",
    "learning_rate = 0.0001\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_vit.parameters(), lr=learning_rate)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "10de4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_vit, train_data_loader, optimizer, epoch):\n",
    "    model_vit.train()\n",
    "    for batch_idx, (img,target) in enumerate(train_data_loader):\n",
    "        img, target = img.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_vit(img)\n",
    "        loss = F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 2 == 0 :\n",
    "            print('train epoch : {}[{}/{} ({: .0f}%)]\\tloss:{:.6f}'.format(epoch, len(img), len(train_data_loader.dataset),\n",
    "                                                                           100* batch_idx / len(train_data_loader.dataset),loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe5a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ed6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941c7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5021a5dc",
   "metadata": {},
   "source": [
    "### utils - helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f63289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model, model_name=None, weights_path=None, load_first_conv=True, load_fc=True, load_repr_layer=False, \n",
    "                           resize_positional_embedding=False, verbose = True, strict=True,):\n",
    "    assert bool(model_name) ^ bool(weights_path), 'expected exactly one of model_name or weights_path'\n",
    "    \n",
    "    # load or download weights\n",
    "    if weights_path is None:\n",
    "        url = PRETRAINED_MODELS[model_name]['url']\n",
    "        if url:\n",
    "            state_dict = model_zoo.load_url(url)   # model_zoo.load_url - 주어진 url에서 torch 직렬화된 개체를 로드\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'pretrained model for {model_name} has not yet been released')   # raise - 예외 에러 발생 시키기, ValueError은 키워드\n",
    "            \n",
    "    else:\n",
    "        state_dict = torch.load(weights_path)\n",
    "        \n",
    "    # modifications to load partial state dict(부분 state dict을 load하기 위한 수정)\n",
    "    expected_missing_keys = []\n",
    "    if not load_first_conv and 'patch_embedding.weight' in state_dict:\n",
    "        expected_missing_keys +=['patch_embedding.weight', 'patch_embedding.bias']\n",
    "    \n",
    "    if not load_fc and 'fc.weight' in state_dict:\n",
    "        expected_missing_keys +=['fc_weight', 'fc.bias']\n",
    "        \n",
    "    if not load_repr_layer and 'pre_logits.weight' in state_dict:\n",
    "        expected_missing_keys +=['pre_logits.weight', 'pre_logits.bias']\n",
    "        \n",
    "    for key in expected_missing_keys:\n",
    "        state_dict.pop(key)    # pop - 리스트의 마지막 요소 꺼내고 삭제\n",
    "        \n",
    "    # change size of positional embeddings\n",
    "    if resize_positional_embedding:\n",
    "        posemb = state_dict['positional_embeddings.pos_embedding']   # static_dict은 torch.save처럼 모델 저장, 각 layer마다 텐서로 매핑되는 매개변수(예를 들어 가중치, 편향 등)를 python dictionary 타입으로 저장한 객체(한마디로 모델 구조에 맞게 각 레이어마다 매개변수를 텐서형태로 매핑해서 dictionary 형태로 저장하는 것)\n",
    "        posemb_new = model.state_dict()['positional_embedding.pos_embedding']\n",
    "        state_dict['positional_embedding.pos_embedding'] = \\resize_positional_embedding(posemb=posemb, posemb_new=posemb_new,\n",
    "                                                                                        has_class_token=hasattr(model, 'class_token'))\n",
    "        maybe_print('Resized positional embeddings from {} to {}'. format(posemb.shape, posemb_new.shape), verbose)\n",
    "        \n",
    "    # load state dict (파라미터 값을 모델에게 입히는 작업,,(?))\n",
    "    ret = model.load_state_dict(state_dict, strict=False)\n",
    "    if strict:\n",
    "        assert set(ret.missing_keys) == set(expected_missing_keys), \\'missing keys when loading pretrained weights:{}'.format(ret.missing_keys)\n",
    "        assert not ret.unexpected_keys, \\'missing keys when loading pretrined weights: {}'.format(ret.unexpected_keys)\n",
    "        maybe_print('loaded pretrined weights.', verbose)\n",
    "    \n",
    "    else:\n",
    "        maybe_print('missing keys when loading pretrained weights.{}'.format(ret.missing_keys), verbose)\n",
    "        maybe_print('unexpected keys when loading pretrained weights:{}'.format(ret.unexpected_keys), verbose)\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def resize_positional_embedding:\n",
    "    from scipy.ndimage import zoom\n",
    "    \n",
    "    # deal with class token\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if has_class_token:\n",
    "        posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "        ntok_new -=1\n",
    "        \n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:,:0], posemb[0]\n",
    "        \n",
    "    # get old and new grid sizes\n",
    "    gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "    gs_new = int(np.sqrt(ntok_new))\n",
    "    posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)   # 불러온 그리드로 reshape해주고?\n",
    "    \n",
    "    # rescale grid\n",
    "    zoom_factor = (gs_new / gs_old, gs_new / gs_old,1)\n",
    "    posemb_grid = zoom(posemb_grid, zoom_factor, order =1)   # zoom은 보간해주는거. 줌 패턴으로 그리드를 늘려주고, \n",
    "    posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)  # 그리드 사이즈에 맞게 reshape해주고\n",
    "    posemb_grid = torch.from_numpy(posemb_grid)\n",
    "    \n",
    "    # deal with class token and return \n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    \n",
    "    return posemb\n",
    "\n",
    "def maybe_print(s: str, flag: bool):\n",
    "    if flag:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cd379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c578c316",
   "metadata": {},
   "source": [
    "## 참고\n",
    "\n",
    "https://github.com/FrancescoSaverioZuppichini/ViT\n",
    "    \n",
    "https://github.com/lukemelas/PyTorch-Pretrained-ViT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee34deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch(practice)",
   "language": "python",
   "name": "pytorch_3min"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
