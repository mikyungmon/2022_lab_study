# 2022 study

- ë…¼ë¬¸ ë¦¬ë·° â¡ï¸ notion ë° one noteì— ì •ë¦¬ (notionë§í¬ : https://www.notion.so/paper-study-b6fb5aa331004645a2ac010b5ae5a828 )

- ë…¼ë¬¸ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œ êµ¬í˜„(transformer, DeiT ë“±)

- ê²½ì—° ì°¸ì—¬

## ğŸ’»ìŠ¤í„°ë””í•˜ë©´ì„œ ë°°ìš´ ê²ƒ(ì•Œê²Œëœ ê²ƒ) ## 

ì‚¬ì†Œí•œ ê²ƒì´ë¼ë„ ê¸°ë¡ !

### - Pytorch Lightning

   - Pytorch Lightningì´ë€ pytorch ë¬¸ë²•ì„ ê°€ì§€ë©´ì„œ í•™ìŠµì½”ë“œë¥¼ pytorchë³´ë‹¤ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‘ì„±í•  ìˆ˜ ìˆëŠ” íŒŒì´ì¬ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬
   
   - pytorchë¥¼ í†µí•´ ì‰½ê²Œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆì§€ë§Œ CPU, GPU,TPUê°„ì˜ ë³€ê²½, mixed_precision training(16bit)ë“±ì˜ ë³µì¡í•œ ì¡°ê±´ê³¼ ë°˜ë³µë˜ëŠ” ì½”ë“œ(training, validation,test, inference)ë“¤ì„ì¢€ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ìƒí™” ì‹œí‚¤ìëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ ë‚˜ì˜¤ê²Œë¨

  - ì¦‰, ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ëª¨ë¸ ì½”ë“œì™€ ì—”ì§€ë‹ˆì–´ë§ ì½”ë“œë¥¼ ë¶„ë¦¬í•´ì„œ ì½”ë“œë¥¼ ê¹”ë”í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ í•´ì£¼ê³  16-bit training, ë‹¤ì¤‘ cpu ì‚¬ìš© ë“±ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ í•™ìŠµ ê¸°ë²•ì„ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ì†ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨

  1) install & import
    
     pytorch lightningì„ ì„¤ì¹˜í•˜ê³  importí•¨
     
          pip install pytorch-lightning

          import pytorch_lightning as pl
          
       
  2) lightning model
  
      ê¸°ì¡´ pytorchëŠ” DataLoader, Mode, Optimizer, Training roof ë“±ì„ ì „ë¶€ ë”°ë¡œë”°ë¡œ ì½”ë“œë¡œ êµ¬í˜„í•´ì•¼í–ˆëŠ”ë° pytorch lightningì—ì„œëŠ” Lightning Model Class ì•ˆì— ì´ ëª¨ë“  ê²ƒì„ í•œë²ˆì— êµ¬í˜„í•˜ë„ë¡ ë˜ì–´ìˆìŒ(í´ë˜ìŠ¤ ë‚´ë¶€ì— ìˆëŠ” í•¨ìˆ˜ëª…ì€ ê·¸ëŒ€ë¡œ ì¨ì•¼í•˜ê³  ëª©ì ì— ë§ê²Œ ì¨ì•¼í•¨. ex. Datasetì˜ init, getitem, len)
      
      torchì˜ nn.Moduleê³¼ ê°™ì´ lightning model ì •ì˜ë¥¼ í•  í´ë˜ìŠ¤ì—ëŠ” ë°˜ë“œì‹œ LightningModuleì„ ìƒì† ë°›ìŒ
          
          from efficientnet_pytorch import EfficientNet
          from pytorch_lightning.metrics.classification import AUROC
          from sklearn.metrics import roc_auc_score

          class Model(pl.LightningModule):
              def __init__(self, *args, **kwargs):
                  super().__init__()
                  self.net = EfficientNet.from_pretrained(arch, advprop=True)   # pretrainedëª¨ë¸ ìƒì„±í•˜ê³  transfer learningìœ„í•´ ë§ˆì§€ë§‰ linear layerì¶œë ¥ì„ 1ë¡œ ë°”ê¿”ì¤Œ
                  self.net._fc = nn.Linear(in_features=self.net._fc.in_features, out_features=1, bias=True)
                  
                  
       modelì˜ ì…ë ¥ì— ëŒ€í•œ outputì„ ë‚´ëŠ” forward
      
         def forward(self, x):
              return self.net(x)
                  
       ìµœì í™”ë¥¼ ìœ„í•œ optimizerì™€ learning rate scheduler ì´ˆê¸°í™” ë° ë°˜í™˜    
       
          def configure_optimizers(self):
              optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)
              scheduler = torch.optim.lr_scheduler.OneCycleLR(
                  max_lr=lr,
                  epochs=max_epochs,
                  optimizer=optimizer,
                  steps_per_epoch=int(len(train_dataset) / batch_size),
                  pct_start=0.1,
                  div_factor=10,
                  final_div_factor=100,
                  base_momentum=0.90,
                  max_momentum=0.95,
              )
              return [optimizer], [scheduler]       
  
       forwardí†µí•´ outputì–»ê³  loss ê³„ì‚°í•˜ëŠ” step í•¨ìˆ˜ 
       
       ì—¬ê¸°ì„œ parameter batchëŠ” 1 iterationì— ëŒ€í•œ batchë¥¼ ì˜ë¯¸

          def step(self, batch):  # forward and calculate loss
              # return batch loss
              x, y  = batch
              y_hat = self(x).flatten()
              y_smo = y.float() * (1 - label_smoothing) + 0.5 * label_smoothing
              loss  = F.binary_cross_entropy_with_logits(y_hat, y_smo.type_as(y_hat),
                                                         pos_weight=torch.tensor(pos_weight))
              return loss, y, y_hat.sigmoid()   # y_hat sigmoidì·¨í•´ì„œ 0-1ì‚¬ì´ ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ì¤Œ (ë‚˜ì¤‘ì— accuracyê³„ì‚°ì— ì‚¬ìš©)
  
        1 iterationì— ëŒ€í•œ training
        
        batch ë§Œí¼ outputì„ ì–»ê³  lossì™€ accuracy return
        
           def training_step(self, batch, batch_nb):
              # hardware agnostic training
              loss, y, y_hat = self.step(batch)
              acc = (y_hat.round() == y).float().mean().item()
              tensorboard_logs = {'train_loss': loss, 'acc': acc}
              return {'loss': loss, 'acc': acc, 'log': tensorboard_logs}
  
        validation stepì€ 1 iterationì— ëŒ€í•œ í•¨ìˆ˜ -> training stepê³¼ ê°™ì€ ì—­í• 
        
        validation_epoch_endëŠ” 1 epochì— ëŒ€í•œ í•¨ìˆ˜  -> loggingê³¼ í•™ìŠµ ê³¼ì •ì— ëŒ€í•œ printë¥¼ ìœ„í•´ ì‚¬ìš©
        
        
           def validation_step(self, batch, batch_nb):
                loss, y, y_hat = self.step(batch)
                return {'val_loss': loss,
                        'y': y.detach(), 'y_hat': y_hat.detach()}   # detach() : ê¸°ì¡´ tensorë¥¼ ë³µì‚¬í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜(ê¸°ì¡´ tensorì—ì„œ gradientì „íŒŒê°€ ì•ˆë˜ëŠ” tensor ìƒì„±)

           def validation_epoch_end(self, outputs):  # í•œ ì—í­ì´ ëë‚¬ì„ ë•Œ ì‹¤í–‰
                avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
                y = torch.cat([x['y'] for x in outputs])
                y_hat = torch.cat([x['y_hat'] for x in outputs])
                auc = AUROC()(y_hat, y) if y.float().mean() > 0 else 0.5 # skip sanity check
                acc = (y_hat.round() == y).float().mean().item()
                print(f"Epoch {self.current_epoch} acc:{acc} auc:{auc}")
                tensorboard_logs = {'val_loss': avg_loss, 'val_auc': auc, 'val_acc': acc}
                return {'avg_val_loss': avg_loss,
                  'val_auc': auc, 'val_acc': acc,
                  'log': tensorboard_logs} 
  
        testë‹¨ê³„ë¥¼ ì¶”ë¡  ê³¼ì •ì´ê¸° ë•Œë¬¸ì— ì •ë‹µì´ ì—†ìŒ  
  
           def test_step(self, batch, batch_nb):
                x, _ = batch
                y_hat = self(x).flatten().sigmoid()
                return {'y_hat': y_hat}

           def test_epoch_end(self, outputs):
                y_hat = torch.cat([x['y_hat'] for x in outputs])
                df_test['target'] = y_hat.tolist()
                N = len(glob('submission*.csv'))
                df_test.target.to_csv(f'submission{N}.csv')
                return {'tta': N}  

        ê° í•™ìŠµ ëª¨ë“œì˜ data loaderë¥¼ ì´ˆê¸°í™”  
        
           def train_dataloader(self):
                return DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers,
                                  drop_last=True, shuffle=True, pin_memory=True)

           def val_dataloader(self):
                return DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers,
                                  drop_last=False, shuffle=False, pin_memory=True)

            def test_dataloader(self):
                return DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers,
                                  drop_last=False, shuffle=False, pin_memory=False)  

        ì§€ê¸ˆê¹Œì§€ê°€ LightningModuleë‚´ì—ì„œ ì •ì˜ë˜ëŠ” ë©”ì„œë“œ

        ì´ë ‡ê²Œ ì¬ì •ì˜ë¥¼ í•˜ê³ ë‚˜ë©´ ë°”ë¡œ í•™ìŠµ ê°€ëŠ¥
          
        pytorch lightningì—ì„œì˜ í•™ìŠµ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŒ
          
            checkpoint_callback = pl.callbacks.ModelCheckpoint('{epoch:02d}_{val_auc:.4f}',
                                                              save_top_k=1, monitor='val_auc', mode='max')  # val_aucê°€ ìµœëŒ€ê°’ì´ ë˜ë©´ ì €ì¥ë˜ë„ë¡í•¨                                                 
            trainer = pl.Trainer(
                tpu_cores=tpu_cores,
                gpus=gpus,
                precision=16 if gpus else 32,
                max_epochs=max_epochs,
                num_sanity_val_steps=1 if debug else 0,  # catches any bugs in your validation without having to wait for the first validation check. 
                checkpoint_callback=checkpoint_callback
                )

            trainer.fit(model)    # í•™ìŠµ ì‹œì‘      
  
  â¡ï¸ **pytorch lightningì˜ ì¥ì ì€ ì„¸ë¶€ì ì¸ high-levelì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ ì¢€ ë” ì •ëˆë˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„± ê°€ëŠ¥**
  
  
   **[ì°¸ê³ ]**

   https://visionhong.tistory.com/30
   
### - EDA(Exploratory Data Analysis) íƒìƒ‰ì  ë°ì´í„° ë¶„ì„

   - EDAë€ ? 
   
      â¡ï¸ ìˆ˜ì§‘í•œ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ, ì´ë¥¼ ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ê´€ì°°í•˜ê³  ì´í•´í•˜ëŠ” ê³¼ì •. í•œë§ˆë””ë¡œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ì „ì— ê·¸ë˜í”„ë‚˜ í†µê³„ì ì¸ ë°©ë²•ìœ¼ë¡œ ìë£Œë¥¼ ì§ê´€ì ìœ¼ë¡œ ë°”ë¼ë³´ëŠ” ê³¼ì •ì„ ë§í•¨
  
  - í•„ìš”í•œ ì´ìœ ? 

      1) ë°ì´í„°ì˜ ë¶„í¬ ë° ê°’ì„ ê²€í† í•¨ìœ¼ë¡œì¨ ë°ì´í„°ê°€ í‘œí˜„í•˜ëŠ” í˜„ìƒì„ ë” ì˜ ì´í•´í•˜ê³ , ë°ì´í„°ì— ëŒ€í•œ ì ì¬ì ì¸ ë¬¸ì œë¥¼ ë°œê²¬í•  ìˆ˜ ìˆìŒ. ì´ë¥¼ í†µí•´, ë³¸ê²©ì ì¸ ë¶„ì„ì— ë“¤ì–´ê°€ê¸°ì— ì•ì„œ ë°ì´í„°ì˜ ìˆ˜ì§‘ì„ ê²°ì •í•  ìˆ˜ ìˆìŒ

      2) ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ì‚´í´ë³´ëŠ” ê³¼ì •ì„ í†µí•´ ë¬¸ì œ ì •ì˜ ë‹¨ê³„ì—ì„œ ë¯¸ì³ ë°œìƒí•˜ì§€ ëª»í–ˆì„ ë‹¤ì–‘í•œ íŒ¨í„´ì„ ë°œê²¬í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ì¡´ì˜ ê°€ì„¤ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ê°€ì„¤ì„ ì„¸ìš¸ ìˆ˜ ìˆìŒ

      **ì¦‰, ëª¨ë¸ë§ì— ì•ì„œ ë°ì´í„°ë¥¼ ì‚´í”¼ëŠ” ëª¨ë“  ê³¼ì •ì„ ì˜ë¯¸**

### - f1 score

   - f1 scoreë€?
      
     - ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ëª¨ë¸ í‰ê°€ì§€í‘œ
     
     - precisionê³¼ recallì˜ ì¡°í™”í‰ê· ìœ¼ë¡œ, ì£¼ë¡œ ë¶„ë¥˜ í´ë˜ìŠ¤ ê°„ì˜ ë°ì´í„°ê°€ ë¶ˆê· í˜•ì´ ì‹¬í•  ë•Œ ì‚¬ìš©í•¨ 
     
        *precision(ì •ë°€ë„) - positiveë¡œ ì˜ˆì¸¡í•œ ê²½ìš° ì¤‘ ì‹¤ì œë¡œ positiveì¸ ë¹„ìœ¨. ì¦‰, ì˜ˆì¸¡ê°’ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œê°€ë¥¼ ì˜ë¯¸. TP / (TP+FP)ë¡œ êµ¬í•¨*
        *recall(ì¬í˜„ìœ¨) - ì‹¤ì œ positiveì¸ ê²ƒ ì¤‘ ì˜¬ë°”ë¥´ê²Œ positiveë¥¼ ë§ì¶˜ ê²ƒì˜ ë¹„ìœ¨. ì¦‰, ì‹¤ì œ ì •ë‹µì„ ì–¼ë§ˆë‚˜ ë§ì·„ëŠëƒë¥¼ ë§í•¨*
     
     - ì •í™•ë„ì˜ ê²½ìš° ë°ì´í„° ë¶„ë¥˜ í´ë˜ìŠ¤ê°€ ê· ì¼í•˜ì§€ ëª»í•˜ë©´ ë¨¸ì‹ ëŸ¬ë‹ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— f1 scoreì‚¬ìš©
     
     - ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸
     
     - ì‹ì€ 2 * (precision * recall / (precision + recall))ê³¼ ê°™ìŒ

### - Albumentation
   
   - Albumentationì€ ì´ë¯¸ì§€ë¥¼ ì†ì‰½ê²Œ augmentationí•´ì£¼ëŠ” python ë¼ì´ë¸ŒëŸ¬ë¦¬

   - pytorchë¥¼ ì˜ˆì‹œë¡œ ë“¤ë©´ torchviwionì´ë¼ëŠ” íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ transformì„ í•  ìˆ˜ ìˆë„ë¡ ì§€ì›ì„ í•˜ëŠ”ë°, albumentationì€ torchvisionì—ì„œ ì§€ì›í•˜ëŠ” transformë³´ë‹¤ ë” íš¨ìœ¨ì ì´ê³  ë‹¤ì–‘í•œ augmentationê¸°ë²•ì„ ì§€ì›í•¨

   ![image](https://user-images.githubusercontent.com/66320010/164391882-1068f301-21ec-45b0-8f0f-f5573e02549a.png)

   - ë‹¤ì–‘í•œ ì˜ìƒë³€í™˜ ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•˜ê³  ìˆê³  ì²˜ë¦¬ì†ë„ë„ ë§¤ìš° ë¹¨ë¼ ë”¥ëŸ¬ë‹ ì „ì²˜ë¦¬ ìš©ìœ¼ë¡œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

   - íŒŒì´ì¬ 3.6 ë²„ì „ ì´ìƒì„ ì‚¬ìš©í•˜ì—¬ì•¼í•˜ê³  ì„¤ì¹˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í•¨
   
         pip install -U albumentations

   - ì‚¬ìš©ë°©ë²•
   
      1) import albumentation ì„ í•œë‹¤.
      2) transform = A.Compose([]) ì„ ì´ìš©í•˜ì—¬ augmentationì„ ì ìš©í•˜ê¸° ìœ„í•œ ê°ì²´ë¥¼ ìƒì„±í•œë‹¤.
      3) augmentations = transform(image=image, mask=mask)ë¥¼ í•˜ì—¬ ì‹¤ì œ augmentationì„ ì ìš©í•œë‹¤.
      4) augmentation_img = augmentations["image"] í•˜ì—¬ augmentationëœ ì´ë¯¸ì§€ë¥¼ ì–»ëŠ”ë‹¤.

   - ì˜ˆì‹œ ì½”ë“œ(ì¶œì²˜: https://gaussian37.github.io/dl-pytorch-albumentation/)

         import albumentations as A
         import cv2

         image = cv2.imread("city_image.png")
         mask = cv2.imread("city_mask.png")

         height = 150
         width = 300

         # Declare an augmentation pipeline
         transform = A.Compose([
             A.Resize(height=height, width=width),
             A.RandomResizedCrop(height=height, width=width, scale=(0.3, 1.0)),
         ])

         augmentations = transform(image=image, mask=mask)
         augmentation_img = augmentations["image"]
         augmentation_mask = augmentations["mask"]

         cv2.imwrite("city_image_augmented.png", augmentation_img)
         cv2.imwrite("city_mask_augmented.png", augmentation_mask)
